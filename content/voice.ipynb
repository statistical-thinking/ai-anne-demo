{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc2b15b0-cfbb-4413-a54e-ea07f37328a9",
   "metadata": {},
   "source": [
    "# AI-ANNE: (A) (N)EURAL (N)ET FOR (E)XPLORATION\n",
    "#### by Prof. Dr. habil. Dennis Klinkhammer (2025)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9dedb26-81bf-4b2f-97e1-580eb8eab74f",
   "metadata": {},
   "source": [
    "![title](img/voice1.png)\n",
    "# About this Tutorial\n",
    "In this tutorial you will learn how a neural network can recognize voices and learns to **differentiate between young and old** voices.\n",
    "# What is a Neural Network?\n",
    "A neural network is a type of computer program that tries to learn things in a way that’s **a bit like how our brains work**. It looks at examples, finds patterns in the data, and then uses those **patterns to make decisions or predictions**. The network is made up of **layers**, and each layer has small units called neurons. These **neurons are connected to each other**, and each connection has a **weight**, which tells the network how important a piece of information is. Each neuron also has a **bias**, which helps shift the result a bit. When information moves through the network, it goes from one layer to the next, and at each step an **activation function** decides whether a neuron should be activated or not, kind of like how our brain decides **which signals to pay attention** to. This whole system works together so the network can learn and improve by adjusting the weights and biases as it practices."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dbac749-739f-419e-8e7b-a2eec8b8625f",
   "metadata": {},
   "source": [
    "# From Measurements to Data\n",
    "When we train a neural network, we need to give it two things: **input data (X)** and the **correct answers (y)**, so it can learn to make predictions. **X** is a **list of examples**, where each example is a list of numbers that describe something. These numbers are called **features**. For example, if you're training a neural network to recognize a specific voice, the features might be things like the peak amplitude or the wave period. So each inner list in X represents a different voice, and each number in that list is a measurement regarding the different voices. Furthermore, **y** is a list of **labels that tell the network what the correct answer** is for each example. So, if the first case in X is a young voice, the first number in y is 0. If the second case is a old voice, the second number in y is 1, and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b269792-bcad-46f9-a3a3-656fa0a09a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features (X = [V, N, M, C]) and Labels (y = [0 / 1)\n",
    "X = [[ 3.4, 1.3, 6.9, 7.1],\n",
    "     [ 2.6, 0.6, 6.4 ,7.6],\n",
    "     [ 3.3, 1.3, 6.8, 7.0],\n",
    "     [ 2.5, 0.6, 6.3, 7.5],\n",
    "     [ 3.2, 1.2, 6.7, 7.0],\n",
    "     [ 2.4, 0.5, 6.3, 7.4],\n",
    "     [ 3.1, 1.2, 6.6, 6.9],\n",
    "     [ 2.3, 0.5, 6.3, 7.3],\n",
    "     [ 3.0, 1.1, 6.5, 6.8],\n",
    "     [ 2.3, 0.5, 6.2, 7.3]]\n",
    "\n",
    "y = [0,1,0,1,0,1,0,1,0,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b22af1-94e2-44e7-b60c-9cee236941ed",
   "metadata": {},
   "source": [
    "# Building and Training a Neural Network\n",
    "In a programming language like MicroPython, a **library** is a collection of **pre-written code** that you can use to make your own programs easier to write. Instead of starting from scratch every time, you can **import a library** that already knows how to do certain tasks. The libraries **random** and **math** are imported to simplify the execution of some functions required for self-learning neural networks. Furthermore, neural networks not only require **activation functions** in order to activate the neurons, but also their derivates. The derivative of a function represents its instantaneous rate of change at a specific point, which enables the training of neural networks. In neural networks, **forward propagation** is the process of passing input data through the network's layers to generate a prediction and **backward propagation**, on the other hand, is the mechanism used to train the network by calculating the error between the prediction and the actual output, and then adjusting the network's weights to minimize that error. This important for the learning ability of a neural network. Furthermore, a **loss function** quantifies the difference between a deep learning model's prediction and the actual outcome, essentially acting as a measure of the model's error. Cross-entropy, a specific type of loss function, is commonly used for classification problems, especially when the model outputs probabilities. Finally, the number of **epochs and the learning rate** need to be specified in MicriPython. In neural networks, an epoch represents one **complete pass of the entire training dataset** through the model. Learning rate determines **how much the model's weights are adjusted** during each update step in the training process. Both are crucial hyperparameters that influence training and model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22dc4797-bcab-4803-9adc-573a349aa79e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# --- (1) Hidden Code ---\n",
    "import micropip\n",
    "await micropip.install('ipywidgets')\n",
    "import random, math\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output, HTML\n",
    "\n",
    "# --- Standardization ---\n",
    "def normalize(X):\n",
    "    transposed = list(zip(*X))\n",
    "    mins = [min(col) for col in transposed]\n",
    "    maxs = [max(col) for col in transposed]\n",
    "    return [[(x_i - min_i) / (max_i - min_i + 1e-9) \n",
    "             for x_i, min_i, max_i in zip(x_row, mins, maxs)] for x_row in X]\n",
    "\n",
    "X = normalize(X)\n",
    "\n",
    "# --- Activation & Loss ---\n",
    "def sigmoid(x): return 1 / (1 + math.exp(-x))\n",
    "def sigmoid_derivative(out): return out * (1 - out)\n",
    "def relu(x): return max(0, x)\n",
    "def relu_derivative(out): return 1 if out > 0 else 0\n",
    "def leaky_relu(x): return x if x > 0 else 0.01 * x\n",
    "def leaky_relu_derivative(out): return 1 if out > 0 else 0.01\n",
    "def tanh(x): return math.tanh(x)\n",
    "def tanh_derivative(out): return 1 - out**2\n",
    "def binary_cross_entropy(pred, y): \n",
    "    epsilon = 1e-7\n",
    "    return - (y * math.log(pred + epsilon) + (1 - y) * math.log(1 - pred + epsilon))\n",
    "def binary_cross_entropy_derivative(pred, y): \n",
    "    epsilon = 1e-7\n",
    "    return -(y / (pred + epsilon)) + (1 - y) / (1 - pred + epsilon)\n",
    "\n",
    "# --- Dense Layer ---\n",
    "def dense_forward(x, w, b, act='relu'):\n",
    "    pres = [sum(x[i] * w[j][i] for i in range(len(x))) + b[j] for j in range(len(w))]\n",
    "    outs = []\n",
    "    for z in pres:\n",
    "        if act == 'sigmoid':\n",
    "            outs.append(sigmoid(z))\n",
    "        elif act == 'relu':\n",
    "            outs.append(relu(z))\n",
    "        elif act == 'leaky_relu':\n",
    "            outs.append(leaky_relu(z))\n",
    "        elif act == 'tanh':\n",
    "            outs.append(tanh(z))\n",
    "    return outs, pres\n",
    "\n",
    "def dense_backward(x, grad_out, out, pre, w, b, act='relu', lr=0.01):\n",
    "    grad_in = [0] * len(x)\n",
    "    for j in range(len(w)):\n",
    "        if act == 'sigmoid':\n",
    "            delta = grad_out[j] * sigmoid_derivative(out[j])\n",
    "        elif act == 'relu':\n",
    "            delta = grad_out[j] * relu_derivative(pre[j])\n",
    "        elif act == 'leaky_relu':\n",
    "            delta = grad_out[j] * leaky_relu_derivative(pre[j])\n",
    "        elif act == 'tanh':\n",
    "            delta = grad_out[j] * tanh_derivative(out[j])\n",
    "        for i in range(len(x)):\n",
    "            grad_in[i] += w[j][i] * delta\n",
    "            w[j][i] -= lr * delta * x[i]\n",
    "        b[j] -= lr * delta\n",
    "    return grad_in\n",
    "\n",
    "# --- Visualisierung ---\n",
    "def html_network_with_connections(layer_sizes, input_dim):\n",
    "    all_layers = [input_dim] + layer_sizes + [1]\n",
    "    neuron_size = 20\n",
    "    margin = 10\n",
    "    column_spacing = 80\n",
    "    row_spacing = neuron_size + 20\n",
    "    radius = neuron_size // 2\n",
    "    total_width = len(all_layers) * column_spacing\n",
    "    total_height = max(all_layers) * row_spacing + 2 * margin\n",
    "    svg = f'<svg width=\"{total_width}\" height=\"{total_height}\" style=\"position:absolute; top:0; left:0;\">'\n",
    "    positions = []\n",
    "    for li, n in enumerate(all_layers):\n",
    "        layer_x = li * column_spacing + column_spacing // 2\n",
    "        layer = []\n",
    "        total_layer_height = (n - 1) * row_spacing\n",
    "        offset_y = (total_height - total_layer_height) // 2\n",
    "        for ni in range(n):\n",
    "            layer_y = offset_y + ni * row_spacing\n",
    "            layer.append((layer_x, layer_y))\n",
    "        positions.append(layer)\n",
    "    for i in range(len(positions)-1):\n",
    "        for x1, y1 in positions[i]:\n",
    "            for x2, y2 in positions[i+1]:\n",
    "                svg += f'<line x1=\"{x1}\" y1=\"{y1}\" x2=\"{x2}\" y2=\"{y2}\" stroke=\"gray\" stroke-width=\"1\" />'\n",
    "    svg += '</svg>'\n",
    "    html = f'''\n",
    "    <style>\n",
    "        .network-wrapper {{\n",
    "            position: relative;\n",
    "            width: {total_width}px;\n",
    "            height: {total_height}px;\n",
    "        }}\n",
    "        .network {{\n",
    "            position: absolute;\n",
    "            top: 0; left: 0;\n",
    "            display: flex;\n",
    "            flex-direction: row;\n",
    "            justify-content: center;\n",
    "            height: 100%;\n",
    "        }}\n",
    "        .layer {{\n",
    "            display: flex;\n",
    "            flex-direction: column;\n",
    "            justify-content: center;\n",
    "            align-items: center;\n",
    "            width: {column_spacing}px;\n",
    "        }}\n",
    "        .neuron {{\n",
    "            width: {neuron_size}px;\n",
    "            height: {neuron_size}px;\n",
    "            border: 2px solid black;\n",
    "            border-radius: 50%;\n",
    "            background-color: #f2f2f2;\n",
    "            margin: 10px 0;\n",
    "            box-sizing: border-box;\n",
    "        }}\n",
    "    </style>\n",
    "    <div class=\"network-wrapper\">\n",
    "        {svg}\n",
    "        <div class=\"network\">\n",
    "    '''\n",
    "    for n_neurons in all_layers:\n",
    "        html += '<div class=\"layer\">'\n",
    "        for _ in range(n_neurons):\n",
    "            html += '<div class=\"neuron\"></div>'\n",
    "        html += '</div>'\n",
    "    html += '</div></div>'\n",
    "    display(HTML(html))\n",
    "\n",
    "# --- Speicher ---\n",
    "trained_model = {}\n",
    "\n",
    "# --- Training ---\n",
    "def train_model(X, y, layer_sizes, activations, epochs, lr):\n",
    "    dims = [len(X[0])] + layer_sizes + [1]\n",
    "    weights, biases = [], []\n",
    "    for i in range(len(dims)-1):\n",
    "        w = [[random.uniform(-0.5, 0.5) for _ in range(dims[i])] for _ in range(dims[i+1])]\n",
    "        b = [random.uniform(-0.5, 0.5) for _ in range(dims[i+1])]\n",
    "        weights.append(w)\n",
    "        biases.append(b)\n",
    "    loss_trace = []\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for xi, yi in zip(X, y):\n",
    "            x = xi\n",
    "            acts, pres = [], []\n",
    "            for i in range(len(weights)):\n",
    "                act = 'sigmoid' if i == len(weights)-1 else activations[i]\n",
    "                x, pre = dense_forward(x, weights[i], biases[i], act)\n",
    "                acts.append(x)\n",
    "                pres.append(pre)\n",
    "            loss = binary_cross_entropy(acts[-1][0], yi)\n",
    "            total_loss += loss\n",
    "            grad = [binary_cross_entropy_derivative(acts[-1][0], yi)]\n",
    "            for i in reversed(range(len(weights))):\n",
    "                act = 'sigmoid' if i == len(weights)-1 else activations[i]\n",
    "                inp = xi if i == 0 else acts[i-1]\n",
    "                grad = dense_backward(inp, grad, acts[i], pres[i], weights[i], biases[i], act, lr)\n",
    "        loss_trace.append(total_loss)\n",
    "    return {\n",
    "        \"weights\": weights,\n",
    "        \"biases\": biases,\n",
    "        \"loss_trace\": loss_trace\n",
    "    }\n",
    "\n",
    "# --- Widgets ---\n",
    "layers_slider = widgets.IntSlider(value=2, min=1, max=5, step=1, description='Layers:')\n",
    "neuron_and_activation_controls = widgets.VBox()\n",
    "epochs_slider = widgets.IntSlider(value=100, min=10, max=500, step=10, description='Epochs:')\n",
    "lr_slider = widgets.FloatSlider(value=0.05, min=0.001, max=1.0, step=0.01, description='L-Rate:')\n",
    "train_button = widgets.Button(description=\"Train\")\n",
    "train_output = widgets.Output()\n",
    "\n",
    "def update_neuron_sliders(*args):\n",
    "    count = layers_slider.value\n",
    "    controls = []\n",
    "    for i in range(count):\n",
    "        neuron_slider = widgets.IntSlider(value=4, min=1, max=10, step=1, description=f'{i+1}. Layer:')\n",
    "        activation_dropdown = widgets.Dropdown(\n",
    "            options=['relu', 'leaky_relu', 'sigmoid', 'tanh'],\n",
    "            value='relu',\n",
    "            description='with function:'\n",
    "        )\n",
    "        controls.append(widgets.HBox([neuron_slider, activation_dropdown]))\n",
    "    neuron_and_activation_controls.children = controls\n",
    "\n",
    "layers_slider.observe(update_neuron_sliders, names='value')\n",
    "update_neuron_sliders()\n",
    "\n",
    "def on_train_click(b):\n",
    "    train_output.clear_output()\n",
    "    layer_sizes = []\n",
    "    activations = []\n",
    "    for control in neuron_and_activation_controls.children:\n",
    "        neuron_slider, activation_dropdown = control.children\n",
    "        layer_sizes.append(neuron_slider.value)\n",
    "        activations.append(activation_dropdown.value)\n",
    "    epochs = epochs_slider.value\n",
    "    lr = lr_slider.value\n",
    "    model = train_model(X, y, layer_sizes, activations, epochs, lr)\n",
    "    trained_model.clear()\n",
    "    trained_model.update({\n",
    "        \"weights\": model[\"weights\"],\n",
    "        \"biases\": model[\"biases\"],\n",
    "        \"loss_trace\": model[\"loss_trace\"],\n",
    "        \"layer_sizes\": layer_sizes\n",
    "    })\n",
    "    with train_output:\n",
    "        html_network_with_connections(layer_sizes, input_dim=len(X[0]))\n",
    "\n",
    "train_button.on_click(on_train_click)\n",
    "\n",
    "display(widgets.VBox([\n",
    "    widgets.HTML(\"<b>1. Number of Layers (N)</b>\"),\n",
    "    layers_slider,\n",
    "    widgets.HTML(\"<b>2. Number of Neurons (N) and their functions</b>\"),\n",
    "    neuron_and_activation_controls,\n",
    "    widgets.HTML(\"<b>3. Number of Epochs (N)</b>\"),\n",
    "    epochs_slider,\n",
    "    widgets.HTML(\"<b>4. Learning Rate (%)</b>\"),\n",
    "    lr_slider,\n",
    "    train_button,\n",
    "    train_output\n",
    "]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539ce189-fcc6-4b34-873e-0b28505cdc4c",
   "metadata": {},
   "source": [
    "# Evaluating the Performance of a Neural Network\n",
    "A **confusion matrix** can be used to evaluate the performance of the neural network. A confusion matrix is a simple table used to check **how well a classification model is working**, especially in binary tasks where there are only two possible outcomes, like “cow” or “rabbit”. It shows how many predictions the model got right and wrong by comparing the predicted labels to the true labels. The table has four parts: true positives (the model correctly said “old voice”), true negatives (it correctly said “young voice”), false positives (it said “old voice” but it was actually “young voice”), and false negatives (it said “young voice” but it was actually “old voice”). This helps us see not just how often the model is right, but also what kinds of mistakes it makes. The **accuracy** is a number that tells us **how often the model makes the correct prediction**. Based on a confusion matrix, accuracy is calculated by adding up all the correct predictions (the true positives and true negatives) and dividing that by the total number of predictions made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db6956a4-2d27-495d-99b9-5eceffb34dcc",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# --- (2) Hidden Code ---\n",
    "from IPython.display import display\n",
    "eval_button = widgets.Button(description=\"Evaluate\")\n",
    "eval_output = widgets.Output()\n",
    "\n",
    "def predict(x, weights, biases):\n",
    "    for i in range(len(weights)):\n",
    "        act = 'sigmoid' if i == len(weights)-1 else 'relu'\n",
    "        x, _ = dense_forward(x, weights[i], biases[i], act)\n",
    "    return 1 if x[0] > 0.5 else 0\n",
    "\n",
    "def on_eval_click(b):\n",
    "    eval_output.clear_output()\n",
    "    if not trained_model:\n",
    "        with eval_output:\n",
    "            print(\"Please train the model first!\")\n",
    "        return\n",
    "\n",
    "    weights = trained_model[\"weights\"]\n",
    "    biases = trained_model[\"biases\"]\n",
    "    losses = trained_model[\"loss_trace\"]\n",
    "\n",
    "    ypred = [predict(xi, weights, biases) for xi in X]\n",
    "    TP = TN = FP = FN = 0\n",
    "    for true, pred in zip(y, ypred):\n",
    "        if true == pred:\n",
    "            if true == 1: TP += 1\n",
    "            else: TN += 1\n",
    "        else:\n",
    "            if true == 1: FN += 1\n",
    "            else: FP += 1\n",
    "    acc = (TP + TN) / len(y)\n",
    "\n",
    "    with eval_output:\n",
    "        print(\"Loss (each 10. Epoch):\")\n",
    "        for i in range(9, len(losses), 10):\n",
    "            print(f\"Epoch {i+1:>3}: Loss = {losses[i]:.4f}\")\n",
    "        print(\"\\nConfusion Matrix:\")\n",
    "        print(f\"TN: {TN}  FP: {FP}\")\n",
    "        print(f\"FN: {FN}  TP: {TP}\")\n",
    "        print(f\"Accuracy: {acc:.2f}\")\n",
    "\n",
    "eval_button.on_click(on_eval_click)\n",
    "display(widgets.VBox([eval_button, eval_output]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1ed50c74-4be1-4933-a1a3-40b985fc6aba",
   "metadata": {},
   "source": [
    "![image.png](img/voice2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa7d5ad1-8498-4d3d-8ce6-6a1840b2481c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
