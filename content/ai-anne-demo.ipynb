{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc2b15b0-cfbb-4413-a54e-ea07f37328a9",
   "metadata": {},
   "source": [
    "# AI-ANNE: (A) (N)EURAL (N)ET FOR (E)XPLORATION\n",
    "#### by Prof. Dr. habil. Dennis Klinkhammer (2025)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9dedb26-81bf-4b2f-97e1-580eb8eab74f",
   "metadata": {},
   "source": [
    "![title](img/cow_rabbit.png)\n",
    "# What is a Neural Network?\n",
    "A neural network is a type of computer program that tries to learn things in a way that’s **a bit like how our brains work**. It looks at examples, finds patterns in the data, and then uses those **patterns to make decisions or predictions**. The network is made up of **layers**, and each layer has small units called neurons. These **neurons are connected to each other**, and each connection has a **weight**, which tells the network how important a piece of information is. Each neuron also has a **bias**, which helps shift the result a bit. When information moves through the network, it goes from one layer to the next, and at each step an **activation function** decides whether a neuron should be activated or not, kind of like how our brain decides **which signals to pay attention** to. This whole system works together so the network can learn and improve by adjusting the weights and biases as it practices."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dbac749-739f-419e-8e7b-a2eec8b8625f",
   "metadata": {},
   "source": [
    "# From Measurements to Data\n",
    "When we train a neural network, we need to give it two things: **input data (X)** and the **correct answers (y)**, so it can learn to make predictions. **X** is a **list of examples**, where each example is a list of numbers that describe something. These numbers are called **features**. For example, if you're training a neural network to recognize cows and rabbits, the features might be things like the length between the eyes or from eyes to mouth. So each inner list in X represents one cow or rabbit, and each number in that list is a measurement regarding that cow or rabbit. Furthermore, **y** is a list of **labels that tell the network what the correct answer** is for each example. So, if the first case in X is a cow, the first number in y is 0. If the second case is a rabbit, the second number in y is 1, and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b269792-bcad-46f9-a3a3-656fa0a09a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features (X) and Labels (y)\n",
    "X = [[ 0.81575475, -0.21746808, -0.12904165, -0.65303909],\n",
    "         [ 0.05761837,  1.59476592,  0.84485761,  1.71304456],\n",
    "         [ 0.96738203,  0.68864892, -0.00730424, -0.41643072],\n",
    "         [ 2.02877297,  0.38660992,  2.06223168,  1.00321947],\n",
    "         [ 1.42226386,  0.99068792,  1.33180724,  0.29339437],\n",
    "         [ 0.81575475,  0.99068792,  1.21006983,  1.4764362 ],\n",
    "         [-1.00377258,  0.38660992, -0.49425387, -0.41643072],\n",
    "         [ 0.05761837, -0.51950708, -0.00730424,  0.29339437],\n",
    "         [ 0.36087292,  0.38660992,  1.08833242,  1.23982783],\n",
    "         [ 0.66412748,  0.38660992,  0.35790798,  1.4764362 ],\n",
    "         [ 0.05761837,  0.08457092,  0.84485761,  0.29339437],\n",
    "         [-0.70051802, -0.51950708,  0.23617057,  0.53000274],\n",
    "         [ 0.20924564, -0.21746808,  0.84485761,  1.00321947],\n",
    "         [-0.24563619,  0.08457092, -0.25077906, -0.65303909],\n",
    "         [-2.06516352, -1.42562408, -1.95510276, -1.59947255],\n",
    "         [-1.15539985, -1.42562408, -1.34641572, -1.36286418],\n",
    "         [ 0.05761837, -1.12358508, -0.00730424, -0.41643072],\n",
    "         [ 0.20924564,  0.08457092, -0.73772869, -0.88964745],\n",
    "         [-0.39726347, -0.51950708,  0.23617057, -0.17982236],\n",
    "         [ 0.5125002 ,  0.08457092, -0.37251647, -0.88964745]]\n",
    "\n",
    "y = [0,1,0,1,1,1,0,1,1,1,1,1,1,0,0,0,0,0,0,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b22af1-94e2-44e7-b60c-9cee236941ed",
   "metadata": {},
   "source": [
    "# Libraries and Activation Functions\n",
    "In a programming language like MicroPython, a **library** is a collection of **pre-written code** that you can use to make your own programs easier to write. Instead of starting from scratch every time, you can **import a library** that already knows how to do certain tasks. The libraries **random** and **math** are imported to simplify the execution of some functions required for self-learning neural networks. Furthermore, neural networks not only require **activation functions** in order to activate the neurons, but also their derivates. The derivative of a function represents its instantaneous rate of change at a specific point, which enables the training of neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a69231f-5ec6-4bc3-b3b0-7c4543f3bde2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "import random\n",
    "import math\n",
    "\n",
    "# Sigmoid\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + math.exp(-x))\n",
    "\n",
    "# Derivate of Sigmoid\n",
    "def sigmoid_derivative(output):\n",
    "    return output * (1 - output)\n",
    "\n",
    "# ReLU\n",
    "def relu(x):\n",
    "    return max(0, x)\n",
    "\n",
    "# Derivate of ReLU\n",
    "def relu_derivative(output):\n",
    "    return 1 if output > 0 else 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a06c05-ed38-4483-9f2e-d4477e400a14",
   "metadata": {},
   "source": [
    "# Data Processing from Input to Output\n",
    "In neural networks, **forward propagation** is the process of passing input data through the network's layers to generate a prediction and **backward propagation**, on the other hand, is the mechanism used to train the network by calculating the error between the prediction and the actual output, and then adjusting the network's weights to minimize that error. This important for the learning ability of a neural network. Furthermore, a **loss function** quantifies the difference between a deep learning model's prediction and the actual outcome, essentially acting as a measure of the model's error. Cross-entropy, a specific type of loss function, is commonly used for classification problems, especially when the model outputs probabilities. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3945ce4-cc5b-4a69-91c8-95894225536d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward Propagation\n",
    "def dense_forward(inputs, weights, biases, activation='relu'):\n",
    "    outputs = []\n",
    "    pre_activations = []\n",
    "    for w, b in zip(weights, biases):\n",
    "        z = sum(i*w_ij for i, w_ij in zip(inputs, w)) + b\n",
    "        pre_activations.append(z)\n",
    "        if activation == 'sigmoid':\n",
    "            outputs.append(sigmoid(z))\n",
    "        elif activation == 'relu':\n",
    "            outputs.append(relu(z))\n",
    "        else:\n",
    "            raise Exception(\"Unknown activation\")\n",
    "    return outputs, pre_activations\n",
    "\n",
    "# Backward Propagation\n",
    "def dense_backward(inputs, grad_outputs, outputs, pre_activations, weights, biases, activation='relu', lr=0.01):\n",
    "    input_grads = [0.0 for _ in range(len(inputs))]\n",
    "    for j in range(len(weights)):\n",
    "        if activation == 'sigmoid':\n",
    "            delta = grad_outputs[j] * sigmoid_derivative(outputs[j])\n",
    "        elif activation == 'relu':\n",
    "            delta = grad_outputs[j] * relu_derivative(pre_activations[j])\n",
    "        else:\n",
    "            raise Exception(\"Unknown activation\")\n",
    "        for i in range(len(inputs)):\n",
    "            input_grads[i] += weights[j][i] * delta\n",
    "            weights[j][i] -= lr * delta * inputs[i]\n",
    "        biases[j] -= lr * delta\n",
    "    return input_grads\n",
    "\n",
    "# Loss Function\n",
    "def binary_cross_entropy(predicted, target):\n",
    "    epsilon = 1e-7\n",
    "    return - (target * math.log(predicted + epsilon) + (1 - target) * math.log(1 - predicted + epsilon))\n",
    "\n",
    "def binary_cross_entropy_derivative(predicted, target):\n",
    "    epsilon = 1e-7\n",
    "    return -(target / (predicted + epsilon)) + (1 - target) / (1 - predicted + epsilon)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b95e1f48-8157-4260-ae43-c4d8cb05fc8d",
   "metadata": {},
   "source": [
    "# Random Initialization of Neural Network\n",
    "AI-ANNE is supposed to learn the weights and biases by itself, therefore, the layers and neurons of the neural network will be **initialized with some random values**. The **architecture of the neural network** consists of four independent variables which will be forwarded to three neurons in the input layer and one neuron in the output layer. This is a very simple neural network that consists of four neurons in two layers with according weights (w1 and w2) and biases (b1 and b2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c10476c1-ab4a-4e1e-b447-371c140fcf04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for Initializing Weights and Biases\n",
    "def init_layer(input_size, output_size):\n",
    "    weights = [[random.uniform(-0.5, 0.5) for _ in range(input_size)] for _ in range(output_size)]\n",
    "    biases = [random.uniform(-0.5, 0.5) for _ in range(output_size)]\n",
    "    return weights, biases\n",
    "\n",
    "# Initialize Weights and Biases\n",
    "w1, b1 = init_layer(4, 3)\n",
    "w2, b2 = init_layer(3, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c4b4e87-26f5-44f4-a4a4-5d410e7f9b8b",
   "metadata": {},
   "source": [
    "# Specifications for Learning Behavior\n",
    "Finally, the number of **epochs and the learning rate** need to be specified in MicriPython. In neural networks, an epoch represents one **complete pass of the entire training dataset** through the model. Learning rate determines **how much the model's weights are adjusted** during each update step in the training process. Both are crucial hyperparameters that influence training and model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55bc95cf-de09-46ea-9ca1-d43951b2dc3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Epochs and Learning Rate for Training\n",
    "epochs = 100\n",
    "lr = 0.05\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    for xi, yi in zip(X, y):\n",
    "        # Forward Pass\n",
    "        out1, pre1 = dense_forward(xi, w1, b1, 'relu')\n",
    "        out2, pre2 = dense_forward(out1, w2, b2, 'sigmoid')\n",
    "        loss = binary_cross_entropy(out2[0], yi)\n",
    "        total_loss += loss\n",
    "\n",
    "        # Backward Pass\n",
    "        dL_dout2 = [binary_cross_entropy_derivative(out2[0], yi)]\n",
    "        dL_dout1 = dense_backward(out1, dL_dout2, out2, pre2, w2, b2, 'sigmoid', lr)\n",
    "        _ = dense_backward(xi, dL_dout1, out1, pre1, w1, b1, 'relu', lr)\n",
    "\n",
    "    if epoch % 10 == 0 or epoch == epochs - 1:\n",
    "        print(f\"Epoch {epoch+1}, Loss: {total_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539ce189-fcc6-4b34-873e-0b28505cdc4c",
   "metadata": {},
   "source": [
    "# Predictions and Confusion Matrix\n",
    "The **outcome of the neural network** can be predicted with the collowing code in MicroPython and a **confusion matrix** can be used to evaluate the performance of the neural network. A confusion matrix is a simple table used to check **how well a classification model is working**, especially in binary tasks where there are only two possible outcomes, like “cow” or “rabbit”. It shows how many predictions the model got right and wrong by comparing the predicted labels to the true labels. The table has four parts: true positives (the model correctly said “cow”), true negatives (it correctly said “rabbit”), false positives (it said “cow” but it was actually “rabbit”), and false negatives (it said “rabbit” but it was actually “cow”). This helps us see not just how often the model is right, but also what kinds of mistakes it makes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11da4a33-48e5-476d-b37a-a5e6e48ec602",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction\n",
    "def predict(x):\n",
    "    out1, _ = dense_forward(x, w1, b1, 'relu')\n",
    "    out2, _ = dense_forward(out1, w2, b2, 'sigmoid')\n",
    "    return 1 if out2[0] > 0.5 else 0\n",
    "\n",
    "ypred = [predict(xi) for xi in X]\n",
    "\n",
    "# Confusion Matrix\n",
    "def classification_report(ytrue, ypred):\n",
    "    TP = TN = FP = FN = 0\n",
    "    for true, pred in zip(ytrue, ypred):\n",
    "        if true == pred:\n",
    "            if true == 1:\n",
    "                TP += 1\n",
    "            else:\n",
    "                TN += 1\n",
    "        else:\n",
    "            if true == 1:\n",
    "                FN += 1\n",
    "            else:\n",
    "                FP += 1\n",
    "    accuracy = (TP + TN) / len(ytrue)\n",
    "    print(\"Accuracy: {:.3f}\".format(accuracy))\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(\"TN: {}, FP: {}\".format(TN, FP))\n",
    "    print(\"FN: {}, TP: {}\".format(FN, TP))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c6c0404-d36f-43b4-8f5c-594a45fe6fa0",
   "metadata": {},
   "source": [
    "# Performance of the Neural Network\n",
    "Finally, the **performance** AI-ANNE can be inspected via the confusion matrix. The **accuracy** is a number that tells us **how often the model makes the correct prediction**. Based on a confusion matrix, accuracy is calculated by adding up all the correct predictions (the true positives and true negatives) and dividing that by the total number of predictions made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32279623-c72c-4990-86b3-dbba92d9851f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions\n",
    "ypred = [predict(xi) for xi in X]\n",
    "\n",
    "# Show classification metrics\n",
    "classification_report(y, ypred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1bdac72-ff22-4341-87ea-99f76ee0898c",
   "metadata": {},
   "source": [
    "# Predictions for Validation\n",
    "If the neural network has a **satisfactory level of accuracy**, individual **new cases can be predicted** using the following command in MicroPython. Each correct prediction serves as validation of the functionality of our neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d69c52c-981c-494a-953f-cdcdf1e5e5b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict Cases and Neural Network Validation\n",
    "validate = [[ 0.81575475, -0.21746808, -0.12904165, -0.65303909]]\n",
    "yval = [predict(xi) for xi in validate]\n",
    "yval"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1ed50c74-4be1-4933-a1a3-40b985fc6aba",
   "metadata": {},
   "source": [
    "![image.png](img/mixed_cow_rabbit.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa7d5ad1-8498-4d3d-8ce6-6a1840b2481c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
