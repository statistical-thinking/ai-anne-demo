{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc2b15b0-cfbb-4413-a54e-ea07f37328a9",
   "metadata": {},
   "source": [
    "# AI-ANNE: (A) (N)EURAL (N)ET FOR (E)XPLORATION\n",
    "#### by Prof. Dr. habil. Dennis Klinkhammer (2025)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9dedb26-81bf-4b2f-97e1-580eb8eab74f",
   "metadata": {},
   "source": [
    "![title](cow_rabbit.png)\n",
    "# What is a Neural Network?\n",
    "A neural network consists of **neurons and layers** that process data via **activation functions**. Weights and biases are necessary in order to activate a neuron and to reach out for other neurons in another layer. AI-ANNE will adjust weights and biases automatically in order to identify and process **non-linear patterns** within datasets. As a result, AI-ANNE is a self-learning neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dbac749-739f-419e-8e7b-a2eec8b8625f",
   "metadata": {},
   "source": [
    "# Your Data\n",
    "AI-ANNE will learn the **difference between cows and rabbits** by using **four features**. Features are individual pieces of information (e.g. variables) that the neural network uses to make predictions or decisions. Additionaly, **one dependent variable** differs between cows (0) and rabbits (1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b269792-bcad-46f9-a3a3-656fa0a09a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features (X) and Dependent Variable (y)\n",
    "X = [[ 0.81575475, -0.21746808, -0.12904165, -0.65303909],\n",
    "         [ 0.05761837,  1.59476592,  0.84485761,  1.71304456],\n",
    "         [ 0.96738203,  0.68864892, -0.00730424, -0.41643072],\n",
    "         [ 2.02877297,  0.38660992,  2.06223168,  1.00321947],\n",
    "         [ 1.42226386,  0.99068792,  1.33180724,  0.29339437],\n",
    "         [ 0.81575475,  0.99068792,  1.21006983,  1.4764362 ],\n",
    "         [-1.00377258,  0.38660992, -0.49425387, -0.41643072],\n",
    "         [ 0.05761837, -0.51950708, -0.00730424,  0.29339437],\n",
    "         [ 0.36087292,  0.38660992,  1.08833242,  1.23982783],\n",
    "         [ 0.66412748,  0.38660992,  0.35790798,  1.4764362 ],\n",
    "         [ 0.05761837,  0.08457092,  0.84485761,  0.29339437],\n",
    "         [-0.70051802, -0.51950708,  0.23617057,  0.53000274],\n",
    "         [ 0.20924564, -0.21746808,  0.84485761,  1.00321947],\n",
    "         [-0.24563619,  0.08457092, -0.25077906, -0.65303909],\n",
    "         [-2.06516352, -1.42562408, -1.95510276, -1.59947255],\n",
    "         [-1.15539985, -1.42562408, -1.34641572, -1.36286418],\n",
    "         [ 0.05761837, -1.12358508, -0.00730424, -0.41643072],\n",
    "         [ 0.20924564,  0.08457092, -0.73772869, -0.88964745],\n",
    "         [-0.39726347, -0.51950708,  0.23617057, -0.17982236],\n",
    "         [ 0.5125002 ,  0.08457092, -0.37251647, -0.88964745]]\n",
    "\n",
    "y = [0,1,0,1,1,1,0,1,1,1,1,1,1,0,0,0,0,0,0,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b22af1-94e2-44e7-b60c-9cee236941ed",
   "metadata": {},
   "source": [
    "# Libraries and Activation Functions\n",
    "The libraries **random** and **math** are imported to simplify the execution of some functions required for self-learning neural networks. Self-learning neural networks not only require **activation functions**, but also their derivates. The derivative of a function represents its instantaneous rate of change at a specific point, which enables the training of neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2a69231f-5ec6-4bc3-b3b0-7c4543f3bde2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "import random\n",
    "import math\n",
    "\n",
    "# Sigmoid\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + math.exp(-x))\n",
    "\n",
    "# Derivate of Sigmoid\n",
    "def sigmoid_derivative(output):\n",
    "    return output * (1 - output)\n",
    "\n",
    "# ReLU\n",
    "def relu(x):\n",
    "    return max(0, x)\n",
    "\n",
    "# Derivate of ReLU\n",
    "def relu_derivative(output):\n",
    "    return 1 if output > 0 else 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a06c05-ed38-4483-9f2e-d4477e400a14",
   "metadata": {},
   "source": [
    "# Data Processing \n",
    "In neural networks, **forward propagation** is the process of passing input data through the network's layers to generate a prediction and **backward propagation**, on the other hand, is the mechanism used to train the network by calculating the error between the prediction and the actual output, and then adjusting the network's weights to minimize that error. This important for the learning ability of a neural network. Furthermore, a **loss function** quantifies the difference between a deep learning model's prediction and the actual outcome, essentially acting as a measure of the model's error. Cross-entropy, a specific type of loss function, is commonly used for classification problems, especially when the model outputs probabilities. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c3945ce4-cc5b-4a69-91c8-95894225536d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward Propagation\n",
    "def dense_forward(inputs, weights, biases, activation='relu'):\n",
    "    outputs = []\n",
    "    pre_activations = []\n",
    "    for w, b in zip(weights, biases):\n",
    "        z = sum(i*w_ij for i, w_ij in zip(inputs, w)) + b\n",
    "        pre_activations.append(z)\n",
    "        if activation == 'sigmoid':\n",
    "            outputs.append(sigmoid(z))\n",
    "        elif activation == 'relu':\n",
    "            outputs.append(relu(z))\n",
    "        else:\n",
    "            raise Exception(\"Unknown activation\")\n",
    "    return outputs, pre_activations\n",
    "\n",
    "# Backward Propagation\n",
    "def dense_backward(inputs, grad_outputs, outputs, pre_activations, weights, biases, activation='relu', lr=0.01):\n",
    "    input_grads = [0.0 for _ in range(len(inputs))]\n",
    "    for j in range(len(weights)):\n",
    "        if activation == 'sigmoid':\n",
    "            delta = grad_outputs[j] * sigmoid_derivative(outputs[j])\n",
    "        elif activation == 'relu':\n",
    "            delta = grad_outputs[j] * relu_derivative(pre_activations[j])\n",
    "        else:\n",
    "            raise Exception(\"Unknown activation\")\n",
    "        for i in range(len(inputs)):\n",
    "            input_grads[i] += weights[j][i] * delta\n",
    "            weights[j][i] -= lr * delta * inputs[i]\n",
    "        biases[j] -= lr * delta\n",
    "    return input_grads\n",
    "\n",
    "# Loss Function\n",
    "def binary_cross_entropy(predicted, target):\n",
    "    epsilon = 1e-7\n",
    "    return - (target * math.log(predicted + epsilon) + (1 - target) * math.log(1 - predicted + epsilon))\n",
    "\n",
    "def binary_cross_entropy_derivative(predicted, target):\n",
    "    epsilon = 1e-7\n",
    "    return -(target / (predicted + epsilon)) + (1 - target) / (1 - predicted + epsilon)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b95e1f48-8157-4260-ae43-c4d8cb05fc8d",
   "metadata": {},
   "source": [
    "# Random Initialization of Neural Network\n",
    "Since the neural network is supposed to learn the weights and biases by itself, the layers and neurons of the neural network will be **initialized with some random values**. The **architecture of the neural network** consists of four independent variables which will be forwarded to three neurons in the input layer and one neuron in the output layer. This is a very simple neural network that consists of four neurons in two layers with according weights (w1 and w2) and biases (b1 and b2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c10476c1-ab4a-4e1e-b447-371c140fcf04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for Initializing Weights and Biases\n",
    "def init_layer(input_size, output_size):\n",
    "    weights = [[random.uniform(-0.5, 0.5) for _ in range(input_size)] for _ in range(output_size)]\n",
    "    biases = [random.uniform(-0.5, 0.5) for _ in range(output_size)]\n",
    "    return weights, biases\n",
    "\n",
    "# Initialize Weights and Biases\n",
    "w1, b1 = init_layer(4, 3)\n",
    "w2, b2 = init_layer(3, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c4b4e87-26f5-44f4-a4a4-5d410e7f9b8b",
   "metadata": {},
   "source": [
    "# Learning Behavior\n",
    "Finally, the number of **epochs and the learning rate** need to be specified in MicriPython. In neural networks, an epoch represents one complete pass of the entire training dataset through the model. Learning rate determines how much the model's weights are adjusted during each update step in the training process. Both are crucial hyperparameters that influence training and model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "55bc95cf-de09-46ea-9ca1-d43951b2dc3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 14.7243\n",
      "Epoch 11, Loss: 8.7675\n",
      "Epoch 21, Loss: 2.6471\n",
      "Epoch 31, Loss: 1.2069\n",
      "Epoch 41, Loss: 0.7670\n",
      "Epoch 51, Loss: 0.5462\n",
      "Epoch 61, Loss: 0.4117\n",
      "Epoch 71, Loss: 0.3231\n",
      "Epoch 81, Loss: 0.2615\n",
      "Epoch 91, Loss: 0.2168\n",
      "Epoch 100, Loss: 0.1864\n"
     ]
    }
   ],
   "source": [
    "# Epochs and Learning Rate for Training\n",
    "epochs = 100\n",
    "lr = 0.05\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    for xi, yi in zip(X, y):\n",
    "        # Forward pass\n",
    "        out1, pre1 = dense_forward(xi, w1, b1, 'relu')\n",
    "        out2, pre2 = dense_forward(out1, w2, b2, 'sigmoid')\n",
    "        loss = binary_cross_entropy(out2[0], yi)\n",
    "        total_loss += loss\n",
    "\n",
    "        # Backward pass\n",
    "        dL_dout2 = [binary_cross_entropy_derivative(out2[0], yi)]\n",
    "        dL_dout1 = dense_backward(out1, dL_dout2, out2, pre2, w2, b2, 'sigmoid', lr)\n",
    "        _ = dense_backward(xi, dL_dout1, out1, pre1, w1, b1, 'relu', lr)\n",
    "\n",
    "    if epoch % 10 == 0 or epoch == epochs - 1:\n",
    "        print(f\"Epoch {epoch+1}, Loss: {total_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539ce189-fcc6-4b34-873e-0b28505cdc4c",
   "metadata": {},
   "source": [
    "# Predictions and Confusion Matrix\n",
    "The **outcome of the neural network** can be predicted with the collowing code in MicroPython and a **confusion matrix** can be used to evaluate the performance of the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "11da4a33-48e5-476d-b37a-a5e6e48ec602",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(x):\n",
    "    out1, _ = dense_forward(x, w1, b1, 'relu')\n",
    "    out2, _ = dense_forward(out1, w2, b2, 'sigmoid')\n",
    "    return 1 if out2[0] > 0.5 else 0\n",
    "\n",
    "ypred = [predict(xi) for xi in X]\n",
    "\n",
    "def classification_report(ytrue, ypred):\n",
    "    TP = TN = FP = FN = 0\n",
    "    for true, pred in zip(ytrue, ypred):\n",
    "        if true == pred:\n",
    "            if true == 1:\n",
    "                TP += 1\n",
    "            else:\n",
    "                TN += 1\n",
    "        else:\n",
    "            if true == 1:\n",
    "                FN += 1\n",
    "            else:\n",
    "                FP += 1\n",
    "    accuracy = (TP + TN) / len(ytrue)\n",
    "    print(\"Accuracy: {:.3f}\".format(accuracy))\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(\"TN: {}, FP: {}\".format(TN, FP))\n",
    "    print(\"FN: {}, TP: {}\".format(FN, TP))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c6c0404-d36f-43b4-8f5c-594a45fe6fa0",
   "metadata": {},
   "source": [
    "# Solution\n",
    "Finally, the **performance** AI-ANNE can be inspected via the confusion matrix. A confusion matrix helps visualize the performance of a classification model by **comparing its predictions against the actual results**. It essentially breaks down the predictions into four categories: true positives (correctly predicted positive cases), true negatives (correctly predicted negative cases), false positives (incorrectly predicted positive cases), and false negatives (incorrectly predicted negative cases). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "32279623-c72c-4990-86b3-dbba92d9851f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.000\n",
      "Confusion Matrix:\n",
      "TN: 10, FP: 0\n",
      "FN: 0, TP: 10\n"
     ]
    }
   ],
   "source": [
    "# Generate predictions\n",
    "ypred = [predict(xi) for xi in X]\n",
    "\n",
    "# Show classification metrics\n",
    "classification_report(y, ypred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
